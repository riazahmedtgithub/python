{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a588e4e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-12-21T19:40:20.436905Z",
     "iopub.status.busy": "2022-12-21T19:40:20.436284Z",
     "iopub.status.idle": "2022-12-21T19:40:20.450201Z",
     "shell.execute_reply": "2022-12-21T19:40:20.449014Z"
    },
    "papermill": {
     "duration": 0.021483,
     "end_time": "2022-12-21T19:40:20.452780",
     "exception": false,
     "start_time": "2022-12-21T19:40:20.431297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a12e1393",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-21T19:40:20.458498Z",
     "iopub.status.busy": "2022-12-21T19:40:20.458151Z",
     "iopub.status.idle": "2022-12-21T19:40:41.776160Z",
     "shell.execute_reply": "2022-12-21T19:40:41.775046Z"
    },
    "papermill": {
     "duration": 21.323265,
     "end_time": "2022-12-21T19:40:41.778303",
     "exception": false,
     "start_time": "2022-12-21T19:40:20.455038",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scrapy\r\n",
      "  Downloading Scrapy-2.7.1-py2.py3-none-any.whl (271 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m271.5/271.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting parsel>=1.5.0\r\n",
      "  Downloading parsel-1.7.0-py2.py3-none-any.whl (14 kB)\r\n",
      "Collecting PyDispatcher>=2.0.5\r\n",
      "  Downloading PyDispatcher-2.0.6.tar.gz (38 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting Twisted>=18.9.0\r\n",
      "  Downloading Twisted-22.10.0-py3-none-any.whl (3.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting itemloaders>=1.0.1\r\n",
      "  Downloading itemloaders-1.0.6-py3-none-any.whl (11 kB)\r\n",
      "Collecting queuelib>=1.4.2\r\n",
      "  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\r\n",
      "Collecting itemadapter>=0.1.0\r\n",
      "  Downloading itemadapter-0.7.0-py3-none-any.whl (10 kB)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from scrapy) (59.8.0)\r\n",
      "Collecting zope.interface>=5.1.0\r\n",
      "  Downloading zope.interface-5.5.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (254 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.2/254.2 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting protego>=0.1.15\r\n",
      "  Downloading Protego-0.2.1-py2.py3-none-any.whl (8.2 kB)\r\n",
      "Collecting tldextract\r\n",
      "  Downloading tldextract-3.4.0-py3-none-any.whl (93 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting cssselect>=0.9.1\r\n",
      "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from scrapy) (21.3)\r\n",
      "Collecting service-identity>=18.1.0\r\n",
      "  Downloading service_identity-21.1.0-py2.py3-none-any.whl (12 kB)\r\n",
      "Requirement already satisfied: pyOpenSSL>=21.0.0 in /opt/conda/lib/python3.7/site-packages (from scrapy) (22.0.0)\r\n",
      "Collecting w3lib>=1.17.0\r\n",
      "  Downloading w3lib-2.1.1-py3-none-any.whl (21 kB)\r\n",
      "Requirement already satisfied: cryptography>=3.3 in /opt/conda/lib/python3.7/site-packages (from scrapy) (37.0.2)\r\n",
      "Requirement already satisfied: lxml>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from scrapy) (4.9.1)\r\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.7/site-packages (from cryptography>=3.3->scrapy) (1.15.0)\r\n",
      "Requirement already satisfied: jmespath>=0.9.5 in /opt/conda/lib/python3.7/site-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from protego>=0.1.15->scrapy) (1.15.0)\r\n",
      "Requirement already satisfied: attrs>=19.1.0 in /opt/conda/lib/python3.7/site-packages (from service-identity>=18.1.0->scrapy) (21.4.0)\r\n",
      "Requirement already satisfied: pyasn1 in /opt/conda/lib/python3.7/site-packages (from service-identity>=18.1.0->scrapy) (0.4.8)\r\n",
      "Requirement already satisfied: pyasn1-modules in /opt/conda/lib/python3.7/site-packages (from service-identity>=18.1.0->scrapy) (0.2.7)\r\n",
      "Collecting hyperlink>=17.1.1\r\n",
      "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting constantly>=15.1\r\n",
      "  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in /opt/conda/lib/python3.7/site-packages (from Twisted>=18.9.0->scrapy) (4.4.0)\r\n",
      "Collecting Automat>=0.8.0\r\n",
      "  Downloading Automat-22.10.0-py2.py3-none-any.whl (26 kB)\r\n",
      "Collecting incremental>=21.3.0\r\n",
      "  Downloading incremental-22.10.0-py2.py3-none-any.whl (16 kB)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->scrapy) (3.0.9)\r\n",
      "Requirement already satisfied: filelock>=3.0.8 in /opt/conda/lib/python3.7/site-packages (from tldextract->scrapy) (3.7.1)\r\n",
      "Collecting requests-file>=1.4\r\n",
      "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\r\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.7/site-packages (from tldextract->scrapy) (3.3)\r\n",
      "Requirement already satisfied: requests>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from tldextract->scrapy) (2.28.1)\r\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.12->cryptography>=3.3->scrapy) (2.21)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.1.0->tldextract->scrapy) (1.26.12)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.1.0->tldextract->scrapy) (2.1.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.1.0->tldextract->scrapy) (2022.9.24)\r\n",
      "Building wheels for collected packages: PyDispatcher\r\n",
      "  Building wheel for PyDispatcher (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.6-py3-none-any.whl size=11958 sha256=ad28403077ea0da3d3a3abbc6a68ffc1776f30f29b145960366661187e570758\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/c9/d6/6a/de198d890277cde60ca3dbebe7ae592d3b381c7d9bb2455f4d\r\n",
      "Successfully built PyDispatcher\r\n",
      "Installing collected packages: PyDispatcher, incremental, constantly, zope.interface, w3lib, queuelib, protego, itemadapter, hyperlink, cssselect, Automat, Twisted, requests-file, parsel, tldextract, service-identity, itemloaders, scrapy\r\n",
      "Successfully installed Automat-22.10.0 PyDispatcher-2.0.6 Twisted-22.10.0 constantly-15.1.0 cssselect-1.2.0 hyperlink-21.0.0 incremental-22.10.0 itemadapter-0.7.0 itemloaders-1.0.6 parsel-1.7.0 protego-0.2.1 queuelib-1.6.2 requests-file-1.5.1 scrapy-2.7.1 service-identity-21.1.0 tldextract-3.4.0 w3lib-2.1.1 zope.interface-5.5.2\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f17af06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-21T19:40:41.791726Z",
     "iopub.status.busy": "2022-12-21T19:40:41.791302Z",
     "iopub.status.idle": "2022-12-21T19:40:43.874728Z",
     "shell.execute_reply": "2022-12-21T19:40:43.873763Z"
    },
    "papermill": {
     "duration": 2.092979,
     "end_time": "2022-12-21T19:40:43.877109",
     "exception": false,
     "start_time": "2022-12-21T19:40:41.784130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 19:40:42 [scrapy.utils.log] INFO: Scrapy 2.7.1 started (bot: scrapybot)\n",
      "2022-12-21 19:40:42 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53) - [GCC 9.4.0], pyOpenSSL 22.0.0 (OpenSSL 1.1.1q  5 Jul 2022), cryptography 37.0.2, Platform Linux-5.15.65+-x86_64-with-debian-bullseye-sid\n",
      "2022-12-21 19:40:42 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2022-12-21 19:40:42 [py.warnings] WARNING: /opt/conda/lib/python3.7/site-packages/scrapy/utils/request.py:231: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2022-12-21 19:40:42 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
      "2022-12-21 19:40:42 [scrapy.extensions.telnet] INFO: Telnet Password: 63772998d0c96d1a\n",
      "2022-12-21 19:40:42 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-12-21 19:40:42 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-12-21 19:40:42 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-12-21 19:40:42 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-12-21 19:40:42 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-12-21 19:40:42 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-12-21 19:40:42 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-12-21 19:40:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://info.cern.ch/hypertext/WWW/TheProject.html> (referer: None)\n",
      "2022-12-21 19:40:43 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-12-21 19:40:43 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 245,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 2431,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'elapsed_time_seconds': 0.927165,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 12, 21, 19, 40, 43, 864613),\n",
      " 'log_count/DEBUG': 2,\n",
      " 'log_count/INFO': 10,\n",
      " 'log_count/WARNING': 1,\n",
      " 'memusage/max': 248655872,\n",
      " 'memusage/startup': 248655872,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2022, 12, 21, 19, 40, 42, 937448)}\n",
      "2022-12-21 19:40:43 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title is \"What's out there?\" And the description is \"Pointers to theworld's online information, subjects, W3 servers, etc.\"\n",
      "Title is \"Help\" And the description is \"on the browser you are using\"\n",
      "Title is \"Software Products\" And the description is \"A list of W3 projectcomponents and their current state.(e.g. Line Mode ,X11 Viola ,  NeXTStep, Servers , Tools , Mail robot ,\"\n",
      "Title is \"Technical\" And the description is \"Library ) Details of protocols, formats,program internals etc\"\n",
      "Title is \"Bibliography\" And the description is \"Paper documentationon  W3 and references.\"\n",
      "Title is \"People\" And the description is \"A list of some people involvedin the project.\"\n",
      "Title is \"History\" And the description is \"A summary of the historyof the project.\"\n",
      "Title is \"How can I help\" And the description is \"If you would liketo support the web..\"\n",
      "Title is \"Getting code\" And the description is \"Getting the code byanonymous FTP , etc.\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create the Spider class\n",
    "class MySpider(scrapy.Spider):\n",
    "  name = 'myspider'\n",
    "  # start_requests method\n",
    "  def start_requests(self):\n",
    "    yield scrapy.Request(url = \"http://info.cern.ch/hypertext/WWW/TheProject.html\", callback = self.parse)\n",
    "      \n",
    "  def parse(self, response):\n",
    "    # Parse method\n",
    "    titles = response.xpath('//dt//a/text()').extract()\n",
    "    descrip = ''.join(response.xpath('//dd/text() | //dd//a/text()').extract())\n",
    "    # Splitting by \\n character\n",
    "    descrip1=descrip.split('\\n')\n",
    "    # Replacing the \\n character by space\n",
    "    res=[]\n",
    "    for i in descrip1:\n",
    "      res.append(i.replace('\\n',' '))\n",
    "    # String manipulation technique to merge the elements in list, to get the correct output \n",
    "    res[0:3] = [''.join(res[0:3])]\n",
    "    res[2:5] = [''.join(res[2:5])]\n",
    "    res[2:4] = [''.join(res[2:4])]\n",
    "    res[3:5] = [''.join(res[3:5])]\n",
    "    res[3:5] = [''.join(res[3:5])]\n",
    "    res[4:6] = [''.join(res[4:6])]\n",
    "    res[5:7] = [''.join(res[5:7])]\n",
    "    res[6:8] = [''.join(res[6:8])]\n",
    "    res[7:9] = [''.join(res[7:9])]\n",
    "    res[8:] = [''.join(res[8:])]\n",
    "    # Zipping both the lists and using for to loop over and print it\n",
    "    for titles,descrip in zip(titles,res): \n",
    "      print(\"Title is \" + \"\\\"\" + str(titles) + \"\\\"\" + \" And the description is \" + \"\\\"\" + str(descrip).strip() + \"\\\"\")\n",
    "# Run the Spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(MySpider)\n",
    "process.start()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 32.532499,
   "end_time": "2022-12-21T19:40:44.707297",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-12-21T19:40:12.174798",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
